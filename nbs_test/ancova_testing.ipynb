{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import proteomics_downstream_analysis as pda\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "            'Protein.Ids': ['P123', 'P234', 'P345', 'P456', 'P567',\n",
    "                            'P232', 'P124', 'P214', 'P352', 'P109'],\n",
    "            'WT1': [1.0, 1.0, 1, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO1': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],   \n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bl_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# read data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbl_data.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filepath)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mGenes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mGenes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tungvuduc/Desktop/PhD/projects/proteomics_downstream_analysis/nbs_test/ancova_testing.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mProtein.Ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mProtein.Ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiannOOP/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiannOOP/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiannOOP/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiannOOP/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiannOOP/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bl_data.csv'"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "filepath = 'bl_data.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "data['Genes'] = data['Genes'].astype('string')\n",
    "data['Protein.Ids'] = data['Protein.Ids'].astype('string')\n",
    "\n",
    "# read the contamination panel\n",
    "filepath = '/Users/tungvuduc/Desktop/PhD/projects/PPMI/urine_analysis/contam_panel.xlsx'\n",
    "panel = pd.read_excel(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read meta data\n",
    "# # Path\n",
    "folder = \"/Users/tungvuduc/Desktop/PhD/projects/PPMI/meta_data/meta_data_for_analysis/feature_engineering_meta_data\"\n",
    "\n",
    "files = {os.path.splitext(file)[0]: pd.read_csv(os.path.join(folder, file)) \n",
    "              for file in os.listdir(folder) if file.endswith('.csv') \n",
    "              and not file.startswith('code_decode')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['educ_data',\n",
       " 'schwab_england_data',\n",
       " 'updrs_data',\n",
       " 'age_data_long',\n",
       " 'demo_data',\n",
       " 'disease_state',\n",
       " 'participant_info',\n",
       " 'gen_data',\n",
       " 'updrs_data_long',\n",
       " 'fam_hist_data',\n",
       " 'age_data',\n",
       " 'schwab_england_data_long']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change patno to string\n",
    "for key in files.keys():\n",
    "    files[key]['PATNO'] = files[key]['PATNO'].astype('string')\n",
    "    \n",
    "list(files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables with different meta data\n",
    "# merge cohort and mutation columns\n",
    "cohort_mut_meta = files['disease_state'].merge(files['participant_info'], on='PATNO')\n",
    "cohort_mut_meta['cohort_lrrk2_mutation'] = cohort_mut_meta['disease_state'] + ' & ' + cohort_mut_meta['LRRK2 mutation']\n",
    "cohort_mut_meta['cohort_gba_mutation'] = cohort_mut_meta['disease_state'] + ' & ' + cohort_mut_meta['GBA mutation']\n",
    "\n",
    "# create pda object and add data\n",
    "obj = pda.DiannData()\n",
    "\n",
    "for i in cohort_mut_meta.columns[1:]:\n",
    "    obj.data = data.copy()\n",
    "    obj.update_col_names(cohort_mut_meta, 'PATNO', i)\n",
    "    obj.datasets.append(obj.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These patno are missing: ['3069', '56886']\n"
     ]
    }
   ],
   "source": [
    "# merge meta data\n",
    "# impute missing data in age meta data\n",
    "patno_age_data = files['age_data']['PATNO'].tolist()\n",
    "patno = data.columns[2:].tolist()\n",
    "\n",
    "missing_patno = [i for i in patno if i not in patno_age_data]\n",
    "print(f'These patno are missing: {missing_patno}')\n",
    "\n",
    "files['age_data'].loc[978] = ['3069', 'BL', np.mean(files['age_data']['AGE_AT_VISIT'])]\n",
    "files['age_data'].loc[979] = ['56886', 'BL', np.mean(files['age_data']['AGE_AT_VISIT'])]\n",
    "\n",
    "# merge meta data\n",
    "datasets = [files['age_data'][['PATNO', 'AGE_AT_VISIT']],\n",
    "            files['demo_data'][['PATNO','sex']], \n",
    "            cohort_mut_meta,\n",
    "            files['updrs_data'][files['updrs_data']['Visit ID'] == 'BL'].drop_duplicates(subset = 'PATNO')]\n",
    "\n",
    "meta_data = reduce(lambda x, y: pd.merge(x, y, on='PATNO', how='outer'), datasets)\n",
    "\n",
    "meta_data = meta_data.rename(columns={'AGE_AT_VISIT': 'age'})\n",
    "\n",
    "meta_data = meta_data.set_index('PATNO').reindex(data.columns[2:])\n",
    "obj.meta_data = meta_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: ['PD', 'Prodromal', 'Control']\n",
      "Dataset 2: ['LRRK2+', 'LRRK2-']\n",
      "Dataset 3: ['GBA-', 'GBA+']\n",
      "Dataset 4: ['LRRK2+&GBA-', 'LRRK2+&GBA+', 'LRRK2-&GBA-', 'LRRK2-&GBA+']\n",
      "Dataset 5: ['PD & LRRK2+', 'Prodromal & LRRK2+', 'Control & LRRK2-', 'PD & LRRK2-', 'Prodromal & LRRK2-']\n",
      "Dataset 6: ['PD & GBA-', 'Prodromal & GBA-', 'Prodromal & GBA+', 'PD & GBA+', 'Control & GBA-', 'Control & GBA+']\n"
     ]
    }
   ],
   "source": [
    "# print unique columns in each dataset\n",
    "for idx, df in enumerate(obj.datasets):\n",
    "    print(f'Dataset {idx + 1}: {df.columns.unique().tolist()[2:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with the title \"cohort\" is added\n",
      "Total number of datasets: \"1\"\n",
      "Data with the title \"lrrk2 mutation\" is added\n",
      "Total number of datasets: \"2\"\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "datasets = obj.datasets.copy()\n",
    "obj.datasets = []\n",
    "names = ['cohort', 'lrrk2 mutation', 'gba mutation', 'lrrk2 and gba mutation', 'cohort and lrrk2 mutation', 'cohort and gba mutation']\n",
    "\n",
    "for name, dataset in zip(names[0:2], datasets[:2]):\n",
    "    obj.data = dataset.copy()\n",
    "    obj.preprocessing(method='hybrid',\n",
    "                     completeness=0.5,\n",
    "                     percentage=0.8,\n",
    "                     strategy='mean',\n",
    "                     kind='knn'\n",
    "                     )\n",
    "    obj.add_data(obj.data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rbc and zscore outliers\n",
    "zscore_outliers = [obj.outlier(dataset,'zscore', False)[1] for dataset in obj.datasets] \n",
    "rbc_outliers = [obj.outlier(dataset,'contamination', False, panel)[1] for dataset in obj.datasets]\n",
    "\n",
    "# remove outliers\n",
    "outliers = [(zscore_outliers[i] + rbc_outliers[i]) for i in range(len(obj.datasets))]\n",
    "\n",
    "# get inliers\n",
    "inliers = [~(outlier) for outlier in outliers]\n",
    "\n",
    "# remove outliers from dataset\n",
    "obj.datasets = [dataset.set_index(['Protein.Ids', 'Genes']).iloc[:, inlier] for dataset, inlier in zip(obj.datasets, inliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort meta data\n",
    "meta_data_cohort = obj.meta_data.sort_values('disease_state')\n",
    "meta_data_lrrk2_mut = obj.meta_data.sort_values('LRRK2 mutation')\n",
    "meta_data_gba_mut = obj.meta_data.sort_values('GBA mutation')\n",
    "meta_data_lrrk2_gba_mut = obj.meta_data.sort_values('mutation')\n",
    "meta_data_cohort_lrrk2_mut = obj.meta_data.sort_values('cohort_lrrk2_mutation')\n",
    "meta_data_cohort_gba_mut = obj.meta_data.sort_values('cohort_gba_mutation')\n",
    "\n",
    "#collect meta data\n",
    "meta_datasets = [meta_data_cohort, \n",
    "                 meta_data_lrrk2_mut, \n",
    "                 meta_data_gba_mut,\n",
    "                 meta_data_lrrk2_gba_mut,\n",
    "                 meta_data_cohort_lrrk2_mut,\n",
    "                 meta_data_cohort_gba_mut]\n",
    "\n",
    "# remove outliers in meta data\n",
    "obj.meta_datasets = [dataset.iloc[inlier] for dataset, inlier in zip(meta_datasets, inliers)]\n",
    "\n",
    "# one hot encoding of meta data\n",
    "renamer = {'GBA+':1, 'GBA-':0,\n",
    "           'LRRK2+':1, 'LRRK2-':0}\n",
    "obj.meta_datasets = [dataset.replace(renamer) for dataset in obj.meta_datasets]\n",
    "obj.meta_datasets = [i.reset_index(drop=True) for i in obj.meta_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancova for Control vs Prodromal | Control vs PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:04<00:00, 87.71it/s] \n",
      "100%|██████████| 377/377 [00:04<00:00, 86.23it/s]]\n",
      "100%|██████████| 377/377 [00:04<00:00, 84.93it/s] \n",
      "100%|██████████| 370/370 [00:04<00:00, 83.24it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 85.75it/s] \n",
      "100%|██████████| 377/377 [00:04<00:00, 84.98it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 83.12it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 83.76it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 81.99it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 82.05it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 92.34it/s] \n",
      "100%|██████████| 370/370 [00:04<00:00, 91.70it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 89.34it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 90.54it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 90.72it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 91.34it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 89.14it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 88.76it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 91.56it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 92.06it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = obj.datasets[0].reset_index()\n",
    "cov_data = obj.meta_datasets[0]\n",
    "groups = [['Control', 'Prodromal'],\n",
    "          ['Control', 'PD']]\n",
    "\n",
    "sample_col = 'disease_state'\n",
    "cov = ['sex', 'age']\n",
    "a, b, results = obj.two_tailed_ancova(dataset, cov_data, groups, sample_col, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# create a test_dataset with 4 columns, with the name ['age' 'sex', 'sample', 'ProteinId_1', 'ProteinId_2']\n",
    "data = pd.DataFrame({\n",
    "    'age': [10, 12, 13, 16, 12, 18, 12, 13, 16],\n",
    "    'sex': [0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
    "    'sample': ['Control', 'Control', 'Control',\n",
    "               'Disease1', 'Disease1', 'Disease1',\n",
    "               'Disease2', 'Disease2', 'Disease2'],\n",
    "    'ProteinId_1': [123., 124., 128., 129., 194., 283., 290., 290., 290.,],\n",
    "    'ProteinId_2': [123., 134., 188., 199., 154., 273., 210., 210., 210.,],\n",
    "    'ProteinId_3': [173., 134., 148., 179., 124., 283., 220., 260., 220.,],\n",
    "    'ProteinId_4': [163., 144., 128., 119., 124., 283., 210., 200., 260.,],\n",
    "    'ProteinId_5': [183., 154., 178., 129., 154., 283., 200., 220., 270.,],\n",
    "    'ProteinId_6': [123., 124., 128., 129., 194., 283., 290., 290., 290.,],\n",
    "    'ProteinId_7': [123., 134., 188., 199., 154., 273., 210., 210., 210.,],\n",
    "    'ProteinId_8': [173., 134., 148., 179., 124., 283., 220., 260., 220.,],\n",
    "    'ProteinId_9': [163., 144., 128., 119., 124., 283., 210., 200., 260.,],\n",
    "    'ProteinId_10': [183., 154., 178., 129., 154., 283., 200., 220., 270.,],\n",
    "})\n",
    "\n",
    "results = [pg.ancova(data=data, dv=i, covar=['age', 'sex'], between='sample').iloc[0] for i in data.filter(regex='ProteinId_').columns.tolist()]\n",
    "\n",
    "result = pd.DataFrame(results)\n",
    "pvalues = -np.log10(result['p-unc'])\n",
    "pvalues.tolist()\n",
    "\n",
    "groups = [['Control', 'Disease1'],\n",
    "          ['Control', 'Disease2']]\n",
    "\n",
    "pvalues = []\n",
    "qvalues = []\n",
    "for group in groups:\n",
    "    results = [pg.ancova(data=data[data['sample'].isin(group)], dv=i, covar=['age', 'sex'], between='sample').iloc[0] for i in data.filter(regex='ProteinId_').columns.tolist()]\n",
    "\n",
    "    result = pd.DataFrame(results)\n",
    "    pvalue = -np.log10(result['p-unc'])\n",
    "    pvalues.append(np.array(pvalue))\n",
    "    qvalues.append(fdrcorrection(result['p-unc'])[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65776381, 0.371654  , 0.30992508, 0.34133167, 0.34133167,\n",
       "       0.65776381, 0.371654  , 0.30992508, 0.34133167, 0.34133167,\n",
       "       0.00138658, 0.36977433, 0.19025848, 0.36977433, 0.52634941,\n",
       "       0.00138658, 0.36977433, 0.19025848, 0.36977433, 0.52634941])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalues = np.concatenate(pvalues, axis=0)\n",
    "qvalues = np.concatenate(qvalues, axis=0)\n",
    "qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 109.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 98.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 118.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 119.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 106.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 87.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 112.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 122.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = data.iloc[:, 2:].set_index('sample').T.reset_index(names='Protein.Ids')\n",
    "dataset['Genes'] = dataset['Protein.Ids']\n",
    "cov_data = data.iloc[:, :2]\n",
    "\n",
    "cov = ['age', 'sex']\n",
    "\n",
    "pvals, qvals, results = obj.two_tailed_ancova(dataset, cov_data, groups, 'sample', cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import proteomics_downstream_analysis as pda\n",
    "\n",
    "data1 = pd.DataFrame({\n",
    "            'Protein.Ids': ['P123', 'P234', 'P345', 'P456', 'P567',\n",
    "                            'P232', 'P124', 'P214', 'P352', 'P109'],\n",
    "            'WT1': [1.0, 1.0, 1, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO1': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],   \n",
    "        })\n",
    "\n",
    "def dummy_function(data):\n",
    "    return data.select_dtypes(float)\n",
    "obj = pda.DiannData()\n",
    "\n",
    "pd.concat(obj.paralell_processing(data1, dummy_function), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancova_data = pd.DataFrame({\n",
    "                    'age': [10, 12, 13, 16, 12, 18, 12, 13, 16],\n",
    "                    'sex': [0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
    "                    'sample': ['Control', 'Control', 'Control',\n",
    "                            'Disease1', 'Disease1', 'Disease1',\n",
    "                            'Disease2', 'Disease2', 'Disease2'],\n",
    "                    'ProteinId_1': [123., 124., 128., 129., 194.,\n",
    "                                    283., 290., 290., 290.,],\n",
    "                    'ProteinId_2': [123., 134., 188., 199., 154.,\n",
    "                                    273., 210., 210., 210.,],\n",
    "                    'ProteinId_3': [173., 134., 148., 179., 124.,\n",
    "                                    283., 220., 260., 220.,],\n",
    "                    'ProteinId_4': [163., 144., 128., 119., 124.,\n",
    "                                    283., 210., 200., 260.,],\n",
    "                    'ProteinId_5': [183., 154., 178., 129., 154.,\n",
    "                                    283., 200., 220., 270.,],\n",
    "                    'ProteinId_6': [123., 124., 128., 129., 194.,\n",
    "                                    283., 290., 290., 290.,],\n",
    "                    'ProteinId_7': [123., 134., 188., 199., 154.,\n",
    "                                    273., 210., 210., 210.,],\n",
    "                    'ProteinId_8': [173., 134., 148., 179., 124.,\n",
    "                                    283., 220., 260., 220.,],\n",
    "                    'ProteinId_9': [163., 144., 128., 119., 124.,\n",
    "                                    283., 210., 200., 260.,],\n",
    "                    'ProteinId_10': [183., 154., 178., 129., 154.,\n",
    "                                     283., 200., 220., 270.,],\n",
    "                })\n",
    "        \n",
    "# some preprocessing for ancova data\n",
    "cov_data = ancova_data.iloc[:, :2]\n",
    "\n",
    "ancova_data = ancova_data.iloc[:, 2:].set_index('sample').T.reset_index(names='Protein.Ids')\n",
    "ancova_data['Genes'] = ancova_data['Protein.Ids']\n",
    "cov = ['age', 'sex']\n",
    "groups =[['Control', 'Disease1'], ['Control', 'Disease2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 115.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 138.18it/s]\n"
     ]
    }
   ],
   "source": [
    "pvals, qvals, results = obj.two_tailed_ancova(ancova_data, cov_data, groups, 'sample', cov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_res = np.array(pd.concat([results[0]['-log10 pvalue'], \n",
    "                                        results[1]['-log10 pvalue']],\n",
    "                                        axis=0))\n",
    "pvals = np.array(pvals_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18193002, 0.5267712 , 1.20771329, 0.68867216, 0.81890753,\n",
       "       0.18193002, 0.5267712 , 1.20771329, 0.68867216, 0.81890753,\n",
       "       3.55702539, 0.52897325, 1.11859599, 0.5515604 , 0.27872586,\n",
       "       3.55702539, 0.52897325, 1.11859599, 0.5515604 , 0.27872586])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiannOOP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
