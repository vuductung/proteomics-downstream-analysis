{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import proteomics_downstream_analysis as pda\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "            'Protein.Ids': ['P123', 'P234', 'P345', 'P456', 'P567',\n",
    "                            'P232', 'P124', 'P214', 'P352', 'P109'],\n",
    "            'WT1': [1.0, 1.0, 1, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO1': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],   \n",
    "        })\n",
    "data = obj.paralell_processing(data, dummy_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "filepath = 'bl_data.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "data['Genes'] = data['Genes'].astype('string')\n",
    "data['Protein.Ids'] = data['Protein.Ids'].astype('string')\n",
    "\n",
    "# read the contamination panel\n",
    "filepath = '/Users/tungvuduc/Desktop/PhD/projects/PPMI/urine_analysis/contam_panel.xlsx'\n",
    "panel = pd.read_excel(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read meta data\n",
    "# # Path\n",
    "folder = \"/Users/tungvuduc/Desktop/PhD/projects/PPMI/meta_data/meta_data_for_analysis/feature_engineering_meta_data\"\n",
    "\n",
    "files = {os.path.splitext(file)[0]: pd.read_csv(os.path.join(folder, file)) \n",
    "              for file in os.listdir(folder) if file.endswith('.csv') \n",
    "              and not file.startswith('code_decode')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['educ_data',\n",
       " 'schwab_england_data',\n",
       " 'updrs_data',\n",
       " 'age_data_long',\n",
       " 'demo_data',\n",
       " 'disease_state',\n",
       " 'participant_info',\n",
       " 'gen_data',\n",
       " 'updrs_data_long',\n",
       " 'fam_hist_data',\n",
       " 'age_data',\n",
       " 'schwab_england_data_long']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change patno to string\n",
    "for key in files.keys():\n",
    "    files[key]['PATNO'] = files[key]['PATNO'].astype('string')\n",
    "    \n",
    "list(files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables with different meta data\n",
    "# merge cohort and mutation columns\n",
    "cohort_mut_meta = files['disease_state'].merge(files['participant_info'], on='PATNO')\n",
    "cohort_mut_meta['cohort_lrrk2_mutation'] = cohort_mut_meta['disease_state'] + ' & ' + cohort_mut_meta['LRRK2 mutation']\n",
    "cohort_mut_meta['cohort_gba_mutation'] = cohort_mut_meta['disease_state'] + ' & ' + cohort_mut_meta['GBA mutation']\n",
    "\n",
    "# create pda object and add data\n",
    "obj = pda.DiannData()\n",
    "\n",
    "for i in cohort_mut_meta.columns[1:]:\n",
    "    obj.data = data.copy()\n",
    "    obj.update_col_names(cohort_mut_meta, 'PATNO', i)\n",
    "    obj.datasets.append(obj.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These patno are missing: ['3069', '56886']\n"
     ]
    }
   ],
   "source": [
    "# merge meta data\n",
    "# impute missing data in age meta data\n",
    "patno_age_data = files['age_data']['PATNO'].tolist()\n",
    "patno = data.columns[2:].tolist()\n",
    "\n",
    "missing_patno = [i for i in patno if i not in patno_age_data]\n",
    "print(f'These patno are missing: {missing_patno}')\n",
    "\n",
    "files['age_data'].loc[978] = ['3069', 'BL', np.mean(files['age_data']['AGE_AT_VISIT'])]\n",
    "files['age_data'].loc[979] = ['56886', 'BL', np.mean(files['age_data']['AGE_AT_VISIT'])]\n",
    "\n",
    "# merge meta data\n",
    "datasets = [files['age_data'][['PATNO', 'AGE_AT_VISIT']],\n",
    "            files['demo_data'][['PATNO','sex']], \n",
    "            cohort_mut_meta,\n",
    "            files['updrs_data'][files['updrs_data']['Visit ID'] == 'BL'].drop_duplicates(subset = 'PATNO')]\n",
    "\n",
    "meta_data = reduce(lambda x, y: pd.merge(x, y, on='PATNO', how='outer'), datasets)\n",
    "\n",
    "meta_data = meta_data.rename(columns={'AGE_AT_VISIT': 'age'})\n",
    "\n",
    "meta_data = meta_data.set_index('PATNO').reindex(data.columns[2:])\n",
    "obj.meta_data = meta_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: ['PD', 'Prodromal', 'Control']\n",
      "Dataset 2: ['LRRK2+', 'LRRK2-']\n",
      "Dataset 3: ['GBA-', 'GBA+']\n",
      "Dataset 4: ['LRRK2+&GBA-', 'LRRK2+&GBA+', 'LRRK2-&GBA-', 'LRRK2-&GBA+']\n",
      "Dataset 5: ['PD & LRRK2+', 'Prodromal & LRRK2+', 'Control & LRRK2-', 'PD & LRRK2-', 'Prodromal & LRRK2-']\n",
      "Dataset 6: ['PD & GBA-', 'Prodromal & GBA-', 'Prodromal & GBA+', 'PD & GBA+', 'Control & GBA-', 'Control & GBA+']\n"
     ]
    }
   ],
   "source": [
    "# print unique columns in each dataset\n",
    "for idx, df in enumerate(obj.datasets):\n",
    "    print(f'Dataset {idx + 1}: {df.columns.unique().tolist()[2:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with the title \"cohort\" is added\n",
      "Total number of datasets: \"1\"\n",
      "Data with the title \"lrrk2 mutation\" is added\n",
      "Total number of datasets: \"2\"\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "datasets = obj.datasets.copy()\n",
    "obj.datasets = []\n",
    "names = ['cohort', 'lrrk2 mutation', 'gba mutation', 'lrrk2 and gba mutation', 'cohort and lrrk2 mutation', 'cohort and gba mutation']\n",
    "\n",
    "for name, dataset in zip(names[0:2], datasets[:2]):\n",
    "    obj.data = dataset.copy()\n",
    "    obj.preprocessing(method='hybrid',\n",
    "                     completeness=0.5,\n",
    "                     percentage=0.8,\n",
    "                     strategy='mean',\n",
    "                     kind='knn'\n",
    "                     )\n",
    "    obj.add_data(obj.data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rbc and zscore outliers\n",
    "zscore_outliers = [obj.outlier(dataset,'zscore', False)[1] for dataset in obj.datasets] \n",
    "rbc_outliers = [obj.outlier(dataset,'contamination', False, panel)[1] for dataset in obj.datasets]\n",
    "\n",
    "# remove outliers\n",
    "outliers = [(zscore_outliers[i] + rbc_outliers[i]) for i in range(len(obj.datasets))]\n",
    "\n",
    "# get inliers\n",
    "inliers = [~(outlier) for outlier in outliers]\n",
    "\n",
    "# remove outliers from dataset\n",
    "obj.datasets = [dataset.set_index(['Protein.Ids', 'Genes']).iloc[:, inlier] for dataset, inlier in zip(obj.datasets, inliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort meta data\n",
    "meta_data_cohort = obj.meta_data.sort_values('disease_state')\n",
    "meta_data_lrrk2_mut = obj.meta_data.sort_values('LRRK2 mutation')\n",
    "meta_data_gba_mut = obj.meta_data.sort_values('GBA mutation')\n",
    "meta_data_lrrk2_gba_mut = obj.meta_data.sort_values('mutation')\n",
    "meta_data_cohort_lrrk2_mut = obj.meta_data.sort_values('cohort_lrrk2_mutation')\n",
    "meta_data_cohort_gba_mut = obj.meta_data.sort_values('cohort_gba_mutation')\n",
    "\n",
    "#collect meta data\n",
    "meta_datasets = [meta_data_cohort, \n",
    "                 meta_data_lrrk2_mut, \n",
    "                 meta_data_gba_mut,\n",
    "                 meta_data_lrrk2_gba_mut,\n",
    "                 meta_data_cohort_lrrk2_mut,\n",
    "                 meta_data_cohort_gba_mut]\n",
    "\n",
    "# remove outliers in meta data\n",
    "obj.meta_datasets = [dataset.iloc[inlier] for dataset, inlier in zip(meta_datasets, inliers)]\n",
    "\n",
    "# one hot encoding of meta data\n",
    "renamer = {'GBA+':1, 'GBA-':0,\n",
    "           'LRRK2+':1, 'LRRK2-':0}\n",
    "obj.meta_datasets = [dataset.replace(renamer) for dataset in obj.meta_datasets]\n",
    "obj.meta_datasets = [i.reset_index(drop=True) for i in obj.meta_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancova for Control vs Prodromal | Control vs PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:04<00:00, 87.71it/s] \n",
      "100%|██████████| 377/377 [00:04<00:00, 86.23it/s]]\n",
      "100%|██████████| 377/377 [00:04<00:00, 84.93it/s] \n",
      "100%|██████████| 370/370 [00:04<00:00, 83.24it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 85.75it/s] \n",
      "100%|██████████| 377/377 [00:04<00:00, 84.98it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 83.12it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 83.76it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 81.99it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 82.05it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 92.34it/s] \n",
      "100%|██████████| 370/370 [00:04<00:00, 91.70it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 89.34it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 90.54it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 90.72it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 91.34it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 89.14it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 88.76it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 91.56it/s]\n",
      "100%|██████████| 377/377 [00:04<00:00, 92.06it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = obj.datasets[0].reset_index()\n",
    "cov_data = obj.meta_datasets[0]\n",
    "groups = [['Control', 'Prodromal'],\n",
    "          ['Control', 'PD']]\n",
    "\n",
    "sample_col = 'disease_state'\n",
    "cov = ['sex', 'age']\n",
    "a, b, results = obj.two_tailed_ancova(dataset, cov_data, groups, sample_col, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# create a test_dataset with 4 columns, with the name ['age' 'sex', 'sample', 'ProteinId_1', 'ProteinId_2']\n",
    "data = pd.DataFrame({\n",
    "    'age': [10, 12, 13, 16, 12, 18, 12, 13, 16],\n",
    "    'sex': [0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
    "    'sample': ['Control', 'Control', 'Control',\n",
    "               'Disease1', 'Disease1', 'Disease1',\n",
    "               'Disease2', 'Disease2', 'Disease2'],\n",
    "    'ProteinId_1': [123., 124., 128., 129., 194., 283., 290., 290., 290.,],\n",
    "    'ProteinId_2': [123., 134., 188., 199., 154., 273., 210., 210., 210.,],\n",
    "    'ProteinId_3': [173., 134., 148., 179., 124., 283., 220., 260., 220.,],\n",
    "    'ProteinId_4': [163., 144., 128., 119., 124., 283., 210., 200., 260.,],\n",
    "    'ProteinId_5': [183., 154., 178., 129., 154., 283., 200., 220., 270.,],\n",
    "    'ProteinId_6': [123., 124., 128., 129., 194., 283., 290., 290., 290.,],\n",
    "    'ProteinId_7': [123., 134., 188., 199., 154., 273., 210., 210., 210.,],\n",
    "    'ProteinId_8': [173., 134., 148., 179., 124., 283., 220., 260., 220.,],\n",
    "    'ProteinId_9': [163., 144., 128., 119., 124., 283., 210., 200., 260.,],\n",
    "    'ProteinId_10': [183., 154., 178., 129., 154., 283., 200., 220., 270.,],\n",
    "})\n",
    "\n",
    "results = [pg.ancova(data=data, dv=i, covar=['age', 'sex'], between='sample').iloc[0] for i in data.filter(regex='ProteinId_').columns.tolist()]\n",
    "\n",
    "result = pd.DataFrame(results)\n",
    "pvalues = -np.log10(result['p-unc'])\n",
    "pvalues.tolist()\n",
    "\n",
    "groups = [['Control', 'Disease1'],\n",
    "          ['Control', 'Disease2']]\n",
    "\n",
    "pvalues = []\n",
    "qvalues = []\n",
    "for group in groups:\n",
    "    results = [pg.ancova(data=data[data['sample'].isin(group)], dv=i, covar=['age', 'sex'], between='sample').iloc[0] for i in data.filter(regex='ProteinId_').columns.tolist()]\n",
    "\n",
    "    result = pd.DataFrame(results)\n",
    "    pvalue = -np.log10(result['p-unc'])\n",
    "    pvalues.append(np.array(pvalue))\n",
    "    qvalues.append(fdrcorrection(result['p-unc'])[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65776381, 0.371654  , 0.30992508, 0.34133167, 0.34133167,\n",
       "       0.65776381, 0.371654  , 0.30992508, 0.34133167, 0.34133167,\n",
       "       0.00138658, 0.36977433, 0.19025848, 0.36977433, 0.52634941,\n",
       "       0.00138658, 0.36977433, 0.19025848, 0.36977433, 0.52634941])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalues = np.concatenate(pvalues, axis=0)\n",
    "qvalues = np.concatenate(qvalues, axis=0)\n",
    "qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 109.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 98.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 118.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 119.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 106.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 87.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 112.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 122.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = data.iloc[:, 2:].set_index('sample').T.reset_index(names='Protein.Ids')\n",
    "dataset['Genes'] = dataset['Protein.Ids']\n",
    "cov_data = data.iloc[:, :2]\n",
    "\n",
    "cov = ['age', 'sex']\n",
    "\n",
    "pvals, qvals, results = obj.two_tailed_ancova(dataset, cov_data, groups, 'sample', cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import proteomics_downstream_analysis as pda\n",
    "\n",
    "data1 = pd.DataFrame({\n",
    "            'Protein.Ids': ['P123', 'P234', 'P345', 'P456', 'P567',\n",
    "                            'P232', 'P124', 'P214', 'P352', 'P109'],\n",
    "            'WT1': [1.0, 1.0, 1, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'WT3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO1': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO2': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],\n",
    "            'KO3': [1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "                    1.0, 1.0, 1, 1.0, 1.0],   \n",
    "        })\n",
    "\n",
    "def dummy_function(data):\n",
    "    return data.select_dtypes(float)\n",
    "obj = pda.DiannData()\n",
    "\n",
    "pd.concat(obj.paralell_processing(data1, dummy_function), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_parallelprocessing import TestParallelProcessing as ts\n",
    "\n",
    "obj = ts()\n",
    "obj.setUp()\n",
    "\n",
    "obj.test_split_data_for_parallel_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiannOOP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
